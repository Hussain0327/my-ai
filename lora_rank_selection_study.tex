\documentclass[11pt,a4paper]{article}

% ============== PACKAGES ==============
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{microtype}

\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{geometry}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{natbib}
% Page geometry
\geometry{margin=1in}

% Graphics path (optional)
\graphicspath{{./}{figures/}}

% Hyperlink styling
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    urlcolor=blue,
    citecolor=blue
}

% Table spacing
\renewcommand{\arraystretch}{1.12}

% ============== TITLE ==============
\title{\textbf{Optimizing Financial Sentiment Analysis:\\A Systematic Study of LoRA Rank Selection}}

\author{Raja Hussain$^{1,2}$\thanks{This work was conducted independently. It does not represent the views of New York University.}\\[0.5em]
\small $^1$New York University, New York, NY, USA\\
\small $^2$ValtricAI Research, New York, NY, USA\\
\small \texttt{research@valtric.ai}}

\date{December 2025}

\begin{document}
\maketitle

% ============== ABSTRACT ==============
\begin{abstract}
We present a systematic investigation of Low-Rank Adaptation (LoRA) rank selection for financial sentiment analysis, validated across multiple random seeds ($n=5$ per configuration, 30 total runs). Using DistilRoBERTa-base (82M parameters) on the Twitter Financial News Sentiment dataset, we evaluate full fine-tuning and LoRA at ranks 4, 8, 16, 32, and 64. Our key finding is that \textbf{diminishing returns begin at rank 16}: while transitions from $r=4 \rightarrow 8 \rightarrow 16$ show statistically significant improvements ($p<0.01$), gains from $r=16 \rightarrow 32 \rightarrow 64$ are \textbf{not statistically significant} ($p>0.1$). Notably, the difference between $r=32$ (84.4\%) and $r=64$ (84.6\%) is indistinguishable from noise ($p=0.35$). All LoRA configurations retain 94.6--97.4\% of full fine-tuning accuracy while reducing trainable parameters by 97.8--99.2\%. These results provide evidence-based guidance for practitioners: \textbf{$r=16$ offers the best accuracy within the statistically significant improvement zone}, challenging the common practice of defaulting to higher ranks.
\end{abstract}

\section{Introduction}
\label{sec:intro}

\subsection{Motivation}
Financial sentiment analysis is critical for algorithmic trading, risk assessment, and market intelligence. However, deploying fine-tuned language models at scale presents challenges:

\begin{enumerate}
    \item \textbf{Computational Cost}: Full fine-tuning of transformer models requires substantial GPU memory and training time.
    \item \textbf{Multi-Model Deployment}: Financial institutions often need multiple specialized models (earnings, news, social media).
    \item \textbf{Reproducibility}: Single-run evaluations can be misleading due to random variance.
\end{enumerate}

Low-Rank Adaptation (LoRA) \citep{hu2022lora} has become the dominant parameter-efficient fine-tuning (PEFT) method, reducing trainable parameters by orders of magnitude while maintaining competitive performance. However, the relationship between LoRA rank and downstream task performance remains underexplored, particularly regarding \textit{statistical significance} of observed differences.

\subsection{Research Question}
\textit{What is the optimal LoRA rank for financial sentiment classification, and are differences between rank configurations statistically significant or merely noise?}

\subsection{Contributions}
\begin{enumerate}
    \item A \textbf{multi-seed validated} comparison ($n=5$ per configuration) of LoRA ranks against full fine-tuning for financial sentiment analysis.
    \item Evidence that \textbf{$r=32$ and $r=64$ are statistically indistinguishable} ($p=0.35$), contradicting single-run conclusions.
    \item Identification of \textbf{$r=16$ as the inflection point} where statistically significant improvements cease.
    \item Practical deployment recommendations grounded in statistical evidence rather than point estimates.
\end{enumerate}

\section{Related Work}
\label{sec:related}

\subsection{Parameter-Efficient Fine-Tuning}
\citet{hu2022lora} introduced LoRA, arguing that weight updates during fine-tuning lie in a low-dimensional subspace. By injecting trainable low-rank decomposition matrices, LoRA reduces trainable parameters while preserving performance.

\citet{zhao2024loraland} evaluated LoRA at scale (310 models across 31 tasks), reporting large average gains over base models and strong practical deployment implications.

\subsection{Distributed and Structured LoRA}
\citet{gao2024dlora} proposed DLoRA for distributed PEFT. Their results motivate studying which LoRA capacity (rank) is actually useful for downstream tasks.

\subsection{Financial Sentiment Analysis}
FinBERT \citep{araci2019finbert} established transformer-based approaches for financial NLP. Efficiency-accuracy tradeoffs are increasingly important, but statistically validated rank studies remain limited.

\section{Methodology}
\label{sec:method}

\subsection{Dataset}
We use the \textbf{Twitter Financial News Sentiment} dataset from Hugging Face (\texttt{zeroshot/twitter-financial-news-sentiment}), containing 9,543 labeled samples with three sentiment labels.

\begin{table}[H]
\centering
\caption{Dataset Statistics}
\label{tab:dataset}
\begin{tabular}{lr}
\toprule
\textbf{Split} & \textbf{Samples} \\
\midrule
Train & 7,634 \\
Test & 1,909 \\
\textbf{Total} & \textbf{9,543} \\
\midrule
\multicolumn{2}{l}{\textit{Labels: Bearish (0), Bullish (1), Neutral (2)}} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Model}
We use \textbf{DistilRoBERTa-base} (82M parameters), a distilled RoBERTa variant with 6 transformer layers and 768 hidden dimensions.

\subsection{Multi-Seed Experimental Design}
To reduce variance and support statistical testing, we run each configuration with \textbf{five random seeds}: [42, 123, 456, 789, 1337]. This affects:
\begin{itemize}
    \item Weight initialization (classifier head and LoRA adapters)
    \item Dropout masks during training
    \item Data shuffling order
\end{itemize}
Total experiments: 6 configurations $\times$ 5 seeds = \textbf{30 runs}.

\subsection{Experimental Configurations}

\begin{table}[H]
\centering
\caption{Experimental Configurations}
\label{tab:configs}
\begin{tabular}{llrrl}
\toprule
\textbf{Config} & \textbf{Method} & \textbf{Trainable Params} & \textbf{\% of Total} & \textbf{LoRA $\alpha$} \\
\midrule
1 & Full Fine-Tuning & 82,120,707 & 100.00\% & --- \\
2 & LoRA $r=4$ & 665,859 & 0.81\% & 8 \\
3 & LoRA $r=8$ & 739,587 & 0.90\% & 16 \\
4 & LoRA $r=16$ & 887,043 & 1.08\% & 32 \\
5 & LoRA $r=32$ & 1,181,955 & 1.44\% & 64 \\
6 & LoRA $r=64$ & 1,771,779 & 2.16\% & 128 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{LoRA Configuration:}
\begin{itemize}
    \item Target modules: \texttt{query}, \texttt{value} (attention layers)
    \item Dropout: 0.1
    \item Alpha scaling: $\alpha = 2r$
    \item Bias: None
\end{itemize}

\subsection{Training Setup}

\begin{table}[H]
\centering
\caption{Training Hyperparameters}
\label{tab:hyperparams}
\begin{tabular}{ll}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
Batch Size & 32 \\
Learning Rate (Full FT) & $2 \times 10^{-5}$ \\
Learning Rate (LoRA) & $1 \times 10^{-4}$ \\
Epochs & 3 \\
Max Sequence Length & 128 \\
Optimizer & AdamW \\
Weight Decay & 0.01 \\
Precision & FP16 (mixed) \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Statistical Analysis}
We report mean $\pm$ standard deviation and 95\% confidence intervals (CI) using the t-distribution. For pairwise comparisons, we use \textbf{paired t-tests} (same seed list across configurations) with significance threshold $\alpha = 0.05$.

\section{Results}
\label{sec:results}

\subsection{Multi-Seed Performance Summary}
Table~\ref{tab:results} reports accuracy and weighted F1 across all six configurations.

\begin{table}[H]
\centering
\caption{Multi-Seed Experimental Results ($n=5$ per configuration)}
\label{tab:results}
\begin{tabular}{lrccr}
\toprule
\textbf{Configuration} & \textbf{Trainable (\%)} & \textbf{Accuracy (mean$\pm$std)} & \textbf{95\% CI} & \textbf{F1 (mean$\pm$std)} \\
\midrule
Full Fine-Tuning & 100.00 & \textbf{86.85$\pm$0.70\%} & [85.98, 87.73] & 86.86$\pm$0.69\% \\
\midrule
LoRA $r=4$ & 0.81 & 82.20$\pm$0.83\% & [81.17, 83.23] & 82.12$\pm$0.89\% \\
LoRA $r=8$ & 0.90 & 83.55$\pm$0.55\% & [82.87, 84.23] & 83.57$\pm$0.53\% \\
LoRA $r=16$ & 1.08 & 84.08$\pm$0.61\% & [83.32, 84.83] & 84.11$\pm$0.58\% \\
LoRA $r=32$ & 1.44 & 84.42$\pm$0.89\% & [83.31, 85.53] & 84.49$\pm$0.90\% \\
LoRA $r=64$ & 2.16 & \underline{84.56$\pm$0.76\%} & [83.62, 85.50] & 84.65$\pm$0.73\% \\
\bottomrule
\end{tabular}
\vspace{0.4em}

\footnotesize{\textit{Bold = best overall; Underline = best among LoRA configurations}}
\end{table}

\subsection{Key Finding: Diminishing Returns at $r=16$}
Table~\ref{tab:significance} shows paired t-tests for adjacent rank transitions.

\begin{table}[H]
\centering
\caption{Statistical Significance of Rank Transitions (Paired t-tests)}
\label{tab:significance}
\begin{tabular}{lcccl}
\toprule
\textbf{Transition} & \textbf{$\Delta$ Accuracy} & \textbf{t-statistic} & \textbf{p-value} & \textbf{Significant?} \\
\midrule
$r=4 \rightarrow r=8$ & +1.35 pp & 5.798 & 0.0044 & \textbf{Yes} ** \\
$r=8 \rightarrow r=16$ & +0.52 pp & 8.209 & 0.0012 & \textbf{Yes} ** \\
$r=16 \rightarrow r=32$ & +0.35 pp & 1.855 & 0.1372 & No \\
$r=32 \rightarrow r=64$ & +0.14 pp & 1.065 & 0.3469 & No \\
\bottomrule
\end{tabular}
\vspace{0.4em}

\footnotesize{\textit{Significance: * $p<0.05$, ** $p<0.01$, *** $p<0.001$}}
\end{table}

\textbf{Interpretation:} Accuracy improvements are statistically significant only up to $r=16$. Beyond this point, gains are not distinguishable from random variance.

\subsection{Full Fine-Tuning vs LoRA Comparisons}
\begin{table}[H]
\centering
\caption{Full Fine-Tuning vs Each LoRA Configuration (Paired t-tests)}
\label{tab:fullft_comparison}
\begin{tabular}{lcccc}
\toprule
\textbf{Comparison} & \textbf{$\Delta$ Accuracy} & \textbf{t-statistic} & \textbf{p-value} & \textbf{Significant?} \\
\midrule
Full FT vs $r=4$ & +4.65 pp & 31.124 & $<$0.0001 & \textbf{Yes} *** \\
Full FT vs $r=8$ & +3.30 pp & 18.187 & 0.0001 & \textbf{Yes} *** \\
Full FT vs $r=16$ & +2.78 pp & 17.017 & 0.0001 & \textbf{Yes} *** \\
Full FT vs $r=32$ & +2.43 pp & 11.234 & 0.0004 & \textbf{Yes} *** \\
Full FT vs $r=64$ & +2.29 pp & 12.894 & 0.0002 & \textbf{Yes} *** \\
\bottomrule
\end{tabular}
\end{table}

All LoRA configurations perform significantly worse than full fine-tuning ($p<0.001$), with gaps ranging from 2.29 pp ($r=64$) to 4.65 pp ($r=4$).

\subsection{Run-to-Run Variance}
\begin{table}[H]
\centering
\caption{Run-to-Run Variance Analysis}
\label{tab:variance}
\begin{tabular}{lcccc}
\toprule
\textbf{Configuration} & \textbf{Min} & \textbf{Max} & \textbf{Spread} & \textbf{Std Dev} \\
\midrule
Full Fine-Tuning & 85.96\% & 87.59\% & 1.62 pp & 0.70\% \\
LoRA $r=4$ & 81.25\% & 83.34\% & 2.10 pp & 0.83\% \\
LoRA $r=8$ & 82.77\% & 83.92\% & 1.15 pp & 0.55\% \\
LoRA $r=16$ & 83.18\% & 84.70\% & 1.52 pp & 0.61\% \\
LoRA $r=32$ & 83.18\% & 85.28\% & 2.10 pp & 0.89\% \\
LoRA $r=64$ & 83.55\% & 85.28\% & 1.73 pp & 0.76\% \\
\bottomrule
\end{tabular}
\end{table}

Typical run-to-run variance is approximately 1.5--2.0 percentage points. Single-run differences smaller than this are unreliable without multi-seed validation.

\subsection{Visualization}
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{fig1_lora_rank_multiseed_results.png}
    \caption{Multi-seed LoRA rank study results ($n=5$ seeds per configuration). Key finding: $r=32$ (84.4\%) vs $r=64$ (84.6\%) is not statistically significant ($p=0.35$).}
    \label{fig:results}
\end{figure}

\subsection{Efficiency Analysis}
\begin{table}[H]
\centering
\caption{Efficiency Analysis with Statistical Validation}
\label{tab:efficiency}
\begin{tabular}{lccc}
\toprule
\textbf{Config} & \textbf{Acc. Retention} & \textbf{Param Reduction} & \textbf{Gain vs Previous} \\
\midrule
LoRA $r=4$ & 94.6\% & 99.2\% & --- \\
LoRA $r=8$ & 96.2\% & 99.1\% & Significant ($p=0.004$) \\
LoRA $r=16$ & \textbf{96.8\%} & 98.9\% & Significant ($p=0.001$) \\
LoRA $r=32$ & 97.2\% & 98.6\% & Not significant ($p=0.14$) \\
LoRA $r=64$ & 97.4\% & 97.8\% & Not significant ($p=0.35$) \\
\bottomrule
\end{tabular}
\end{table}

\section{Discussion}
\label{sec:discussion}

\subsection{Why Multi-Seed Validation Changes Conclusions}
Single-run results can flip the apparent ranking between configurations. Multi-seed validation shows that $r=32$ and $r=64$ are statistically indistinguishable ($p=0.35$), so claims that one is definitively better are not supported.

\subsection{Interpretation: Intrinsic Rank Hypothesis}
Results are consistent with the intrinsic-rank hypothesis in LoRA \citep{hu2022lora}: the task appears to have an effective adaptation rank around 16, beyond which additional capacity yields no statistically significant improvement.

\subsection{Why Do Gains Plateau at $r=16$?}
We hypothesize:
\begin{enumerate}
    \item \textbf{Task Complexity}: 3-class sentiment is a constrained downstream task.
    \item \textbf{Dataset Size}: With roughly 7.6K training samples, higher ranks may not learn stable extra structure.
    \item \textbf{Optimization Noise}: Additional degrees of freedom can amplify variance and reduce marginal gains.
\end{enumerate}

\subsection{Practical Deployment Recommendations}
\begin{table}[H]
\centering
\caption{Evidence-Based Deployment Recommendations}
\label{tab:recommendations}
\begin{tabular}{lll}
\toprule
\textbf{Use Case} & \textbf{Recommendation} & \textbf{Rationale} \\
\midrule
Maximum accuracy & Full Fine-Tuning & 2.3--4.6 pp better ($p<0.001$) \\
Production deployment & LoRA $r=16$ & Best within significant-gain zone \\
If rank cost is irrelevant & LoRA $r=32$ or $r=64$ & Marginal, not significant \\
Memory-constrained & LoRA $r=8$ & Strong retention with fewer params \\
Rapid prototyping & LoRA $r=4$ & Fastest experiments \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Limitations}
\begin{enumerate}
    \item \textbf{Single Dataset}: Rank sensitivity may vary across financial corpora.
    \item \textbf{Single Model}: Larger models may exhibit different rank dynamics.
    \item \textbf{Fixed Alpha Scaling}: We used $\alpha = 2r$.
    \item \textbf{Seed Count}: $n=5$ provides moderate power; $n=10+$ would strengthen conclusions.
\end{enumerate}

\section{Conclusion}
\label{sec:conclusion}
We present a multi-seed validated study of LoRA rank selection for financial sentiment analysis. Key findings:
\begin{enumerate}
    \item \textbf{Diminishing returns at $r=16$}: Significant gains occur only up to rank 16.
    \item \textbf{$r=32$ vs $r=64$ is noise}: The 0.14 pp difference is not significant ($p=0.35$).
    \item \textbf{Multi-seed validation is essential}: Single-run differences under 2 pp are unreliable.
    \item \textbf{Practical recommendation}: LoRA $r=16$ is the best default for production efficiency.
\end{enumerate}

\subsection{Future Work}
\begin{enumerate}
    \item Increase seeds to $n=10+$ for stronger statistical power.
    \item Validate across Financial PhraseBank, SEC filings, and earnings transcripts.
    \item Test larger backbones (Mistral-7B, Llama-3-8B) for rank sensitivity.
    \item Explore automated rank selection methods for PEFT.
\end{enumerate}

% ============== REFERENCES ==============
\bibliographystyle{plainnat}

\begin{thebibliography}{10}

\bibitem[Araci(2019)]{araci2019finbert}
Dogu Araci.
\newblock FinBERT: Financial sentiment analysis with pre-trained language models.
\newblock \emph{arXiv preprint arXiv:1908.10063}, 2019.

\bibitem[Gao and Zhang(2024)]{gao2024dlora}
Chao Gao and Sai-Qian Zhang.
\newblock DLoRA: Distributed parameter-efficient fine-tuning solution for large language model.
\newblock In \emph{Findings of the Association for Computational Linguistics: EMNLP 2024}, 2024.

\bibitem[Hu et~al.(2022)]{hu2022lora}
Edward~J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.
\newblock LoRA: Low-rank adaptation of large language models.
\newblock In \emph{International Conference on Learning Representations (ICLR)}, 2022.

\bibitem[Zhao et~al.(2024)]{zhao2024loraland}
Justin Zhao, Timothy Wang, Wael Abid, Geoffrey Angus, Arnav Garg, Jeffery Kinnison, Alex Sherstinsky, Piero Molino, Travis Addair, and Devvret Rishi.
\newblock LoRA Land: 310 fine-tuned LLMs that rival GPT-4, a technical report.
\newblock \emph{arXiv preprint arXiv:2405.00732}, 2024.

\end{thebibliography}

% ============== APPENDIX ==============
\appendix

\section{Individual Run Data}
\label{app:raw}

\begin{table}[H]
\centering
\caption{All 30 Individual Run Results}
\label{tab:individual}
\small
\begin{tabular}{lcccccc}
\toprule
\textbf{Seed} & \textbf{Full FT} & \textbf{$r=4$} & \textbf{$r=8$} & \textbf{$r=16$} & \textbf{$r=32$} & \textbf{$r=64$} \\
\midrule
42   & 87.53\% & 83.34\% & 83.92\% & 84.55\% & 85.49\% & 85.28\% \\
123  & 85.96\% & 81.25\% & 83.24\% & 83.18\% & 83.18\% & 83.55\% \\
456  & 86.43\% & 81.56\% & 82.77\% & 83.55\% & 84.13\% & 84.02\% \\
789  & 86.75\% & 82.29\% & 83.87\% & 84.44\% & 84.86\% & 84.86\% \\
1337 & 87.59\% & 82.56\% & 83.92\% & 84.70\% & 84.44\% & 85.12\% \\
\midrule
\textbf{Mean} & 86.85\% & 82.20\% & 83.55\% & 84.08\% & 84.42\% & 84.56\% \\
\textbf{Std}  & 0.70\%  & 0.83\%  & 0.55\%  & 0.61\%  & 0.89\%  & 0.76\% \\
\bottomrule
\end{tabular}
\end{table}

\section{Reproducibility}
\label{app:repro}

\textbf{Code:} \texttt{lora\_multiseed\_experiment.ipynb}\\
\textbf{Data:} \texttt{multiseed\_aggregated\_results.json}\\
\textbf{Hardware:} NVIDIA GPU with CUDA support\\
\textbf{Framework:} PyTorch, Hugging Face Transformers, PEFT, SciPy

To reproduce:
\begin{enumerate}
    \item Install dependencies: \texttt{pip install transformers datasets peft accelerate scipy}
    \item Open notebook in Jupyter or Google Colab
    \item Select GPU runtime
    \item Run all cells sequentially
\end{enumerate}

\vspace{1em}
\hrule
\vspace{0.5em}
\centering
\textit{ValtricAI Research, December 2025}

\end{document}
